{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset into opencv numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path of the file containing the images\n",
    "PATH = 'Dataset'\n",
    "\n",
    "# Loading all the image paths to count total number of images\n",
    "import pathlib\n",
    "# Loading base path of data\n",
    "data = pathlib.Path(PATH)\n",
    "# Loading all folders from the base path and the files from them\n",
    "stuff = list(data.glob('*/*.jpg'))\n",
    "# Finding the length of the lst, ie; the number of images in total in both folders\n",
    "len(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the files paths into a dictionary with labels\n",
    "files = {'me': [], 'not-me': []}\n",
    "\n",
    "for dir in data.glob('ME/*.jpg'):\n",
    "    files['me'].append(dir)\n",
    "    \n",
    "for dir in data.glob('NOT-ME/*.jpg'):\n",
    "    files['not-me'].append(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([WindowsPath('Dataset/ME/img10.jpg'),\n",
       "  WindowsPath('Dataset/ME/img12.jpg'),\n",
       "  WindowsPath('Dataset/ME/img14.jpg')],\n",
       " [WindowsPath('Dataset/NOT-ME/IMG-20211003-WA0062.jpg'),\n",
       "  WindowsPath('Dataset/NOT-ME/IMG-20211003-WA0068.jpg'),\n",
       "  WindowsPath('Dataset/NOT-ME/IMG-20211004-WA0005.jpg')])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files['me'][:3], files['not-me'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_labels = {'me': 0, 'not-me': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('me',\n",
       " [WindowsPath('Dataset/ME/img10.jpg'),\n",
       "  WindowsPath('Dataset/ME/img12.jpg'),\n",
       "  WindowsPath('Dataset/ME/img14.jpg'),\n",
       "  WindowsPath('Dataset/ME/img16.jpg'),\n",
       "  WindowsPath('Dataset/ME/img18.jpg'),\n",
       "  WindowsPath('Dataset/ME/img2.jpg'),\n",
       "  WindowsPath('Dataset/ME/img20.jpg'),\n",
       "  WindowsPath('Dataset/ME/img22.jpg'),\n",
       "  WindowsPath('Dataset/ME/img24.jpg'),\n",
       "  WindowsPath('Dataset/ME/img26.jpg'),\n",
       "  WindowsPath('Dataset/ME/img28.jpg'),\n",
       "  WindowsPath('Dataset/ME/img30.jpg'),\n",
       "  WindowsPath('Dataset/ME/img32.jpg'),\n",
       "  WindowsPath('Dataset/ME/img34.jpg'),\n",
       "  WindowsPath('Dataset/ME/img36.jpg'),\n",
       "  WindowsPath('Dataset/ME/img38.jpg'),\n",
       "  WindowsPath('Dataset/ME/img4.jpg'),\n",
       "  WindowsPath('Dataset/ME/img40.jpg'),\n",
       "  WindowsPath('Dataset/ME/img42.jpg'),\n",
       "  WindowsPath('Dataset/ME/img44.jpg'),\n",
       "  WindowsPath('Dataset/ME/img46.jpg'),\n",
       "  WindowsPath('Dataset/ME/img6.jpg'),\n",
       "  WindowsPath('Dataset/ME/img8.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132447.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132448.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132448_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132449.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132450.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132451.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132451_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132452.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132452_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132453.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132454.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132454_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132455.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132456.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132456_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132457.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132457_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132458.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132459.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132459_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132500.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132501.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132502.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132502_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132506.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132507.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132507_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132508.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132509.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132510.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132510_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132511.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132511_1.jpg'),\n",
       "  WindowsPath('Dataset/ME/IMG_20220128_132512.jpg')])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(list(files.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the directories as images\n",
    "img_size = [224, 224]               # Do list instead of tuple (this is done to add the third color channel later with east) but convert to tuple in cv2\n",
    "x= []\n",
    "y= []\n",
    "\n",
    "for label, list_dir in files.items():                       # To extract the key and the value from the dictionary\n",
    "    for dir in list_dir:\n",
    "        # print(str(dir))\n",
    "        img = cv.imread(str(dir))\n",
    "        img = cv.resize(img, tuple(img_size))\n",
    "        # Saving the images into x\n",
    "        x.append(img)\n",
    "        # Adding in the label\n",
    "        y.append(files_labels[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'>\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(type(x), type(y))\n",
    "x = np.array(x)\n",
    "y= np.array(y)\n",
    "print(type(x), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing a sample image from the dataset\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.axis('off')\n",
    "plt.imshow(x[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((65, 224, 224, 3), (65,), (28, 224, 224, 3), (28,))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "      rotation_range=10,\n",
    "      shear_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=False,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# train_generator is actually an iterator\n",
    "train_generator = train_datagen.flow(x_train, y_train, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280 280\n"
     ]
    }
   ],
   "source": [
    "x_train_final = []\n",
    "y_train_final = []\n",
    "\n",
    "for i in range(30):\n",
    "    images, labels = train_generator.next()\n",
    "    data = list(zip(images, labels))\n",
    "    for image, label in data:\n",
    "        x_train_final.append(image)\n",
    "        y_train_final.append(label)\n",
    "print(len(x_train_final), len(y_train_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final = np.array(x_train_final)\n",
    "y_train_final = np.array(y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((280, 224, 224, 3), (280,))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train_final), np.shape(y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "# Here we load the ResNet model with the trained weights of imagenet but not includeing the final layer as well will add additional layers\n",
    "# over here to train for our application\n",
    "\n",
    "res_net_base = MobileNet(input_shape= img_size + [3], weights='imagenet', include_top=False)\n",
    "# Supplying weights=\"imagenet\" indicates that we want to use the pre-trained ImageNet weights for the respective model.\n",
    "\n",
    "# Setting all the current layers as non-trainable\n",
    "for layer in res_net_base.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_1.00_224 (Functio  (None, 7, 7, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_max_pooling2d_3 (Glo  (None, 1024)             0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,229,889\n",
      "Trainable params: 1,025\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"res_net.output\" represents the final layer of res_net. We will add additional layers to this layer inorder to add for our situation.\n",
    "\"\"\"\n",
    "from keras.layers import Dense, GlobalMaxPooling2D\n",
    "\n",
    "# Here we will be adding a maxPooling layer)\n",
    "add_layer1 = GlobalMaxPooling2D()\n",
    "\n",
    "# Adding the final dense layer (only 1 as it is a binary class)\n",
    "add_layer2 = Dense(1, activation='sigmoid')\n",
    "\n",
    "# Compiling the model\n",
    "final_model = keras.Sequential([res_net_base, \n",
    "                                add_layer1,\n",
    "                                add_layer2])\n",
    "\n",
    "# Displaying the model summary\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.keras.losses.BinaryCrossentropy\n",
    "\n",
    "final_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(28, 224, 224, 3), (28,)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x_test.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting in the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "28/28 [==============================] - 7s 181ms/step - loss: 1.0410 - accuracy: 0.6536 - val_loss: 0.2080 - val_accuracy: 0.9286\n",
      "Epoch 2/4\n",
      "28/28 [==============================] - 4s 150ms/step - loss: 0.1112 - accuracy: 0.9571 - val_loss: 0.2005 - val_accuracy: 0.9286\n",
      "Epoch 3/4\n",
      "28/28 [==============================] - 4s 148ms/step - loss: 0.0491 - accuracy: 0.9857 - val_loss: 0.1274 - val_accuracy: 0.9643\n",
      "Epoch 4/4\n",
      "28/28 [==============================] - 4s 147ms/step - loss: 0.0242 - accuracy: 0.9964 - val_loss: 0.1768 - val_accuracy: 0.9286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x227c3ac37f0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(x_train_final, y_train_final, epochs = 4, batch_size=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with a random image\n",
    "\n",
    "# img = cv.resize(cv.imread(PATH), tuple(img_size))\n",
    "# plt.imshow(img)\n",
    "# img = img/255\n",
    "# img = np.array([img])\n",
    "# img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0044243]], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_model.predict(img) <0.5:\n",
    "    print(\"Hello Me!!\")\n",
    "else:\n",
    "    print(\"I don't know you!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save(os.path.join('Models', 'Face_MobNets50.h5'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
